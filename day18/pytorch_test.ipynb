{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-05T06:13:40.372533400Z",
     "start_time": "2026-02-05T06:13:40.358686400Z"
    }
   },
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "import torch.nn as nn  # 这行代码就是告诉 Python：以后我写 nn，就是指 torch.nn"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:16:11.256959900Z",
     "start_time": "2026-02-05T06:16:11.245505100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 只有一层：直接从输入到输出\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ],
   "id": "8db9641d929d840e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:16:11.852685600Z",
     "start_time": "2026-02-05T06:16:11.840662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 第一层：输入 -> 隐藏层 (比如 128 个神经元)\n",
    "        self.layer1 = nn.Linear(2, 128)\n",
    "        self.relu = nn.ReLU() # 激活函数 (换成 ReLU 效果更好)\n",
    "\n",
    "        # 第二层：隐藏层 -> 输出\n",
    "        self.layer2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)   # 增加非线性能力\n",
    "        x = self.layer2(x)\n",
    "        return self.sigmoid(x)"
   ],
   "id": "96695c089c707bdf",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T06:18:09.761092200Z",
     "start_time": "2026-02-05T06:18:06.255173400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- 假设你上面已经定义好了 SimpleNeuralNet 类 ---\n",
    "# class SimpleNeuralNet(nn.Module): ... (略)\n",
    "\n",
    "# 1. 准备数据 (这里用一个经典的“异或 XOR”问题)\n",
    "# 逻辑：两数相同为0，不同为1。这是逻辑回归解决不了，但神经网络能解决的问题。\n",
    "X = torch.tensor([[0.0, 0.0],\n",
    "                  [0.0, 1.0],\n",
    "                  [1.0, 0.0],\n",
    "                  [1.0, 1.0]])  # 输入 (4个样本, 每个2维)\n",
    "\n",
    "y = torch.tensor([[0.0],\n",
    "                  [1.0],\n",
    "                  [1.0],\n",
    "                  [0.0]])       # 标签 (4个样本, 每个1维)\n",
    "\n",
    "# 2. 把模型和数据搬到你的 RTX 4080 上\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前使用的设备: {device}\")\n",
    "\n",
    "model = SimpleNeuralNet().to(device)  # 实例化模型并丢进显卡\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "# 3. 定义“裁判”和“教练”\n",
    "# 裁判：损失函数 (BCELoss 用于二分类)\n",
    "criterion = nn.BCELoss()\n",
    "# 教练：优化器 (Adam 通常比 SGD 收敛更快，适合新手)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. 开始训练 (Training Loop)\n",
    "print(\"开始训练...\")\n",
    "for epoch in range(2000):  # 训练 2000 次\n",
    "    # --- 前向传播 (Forward) ---\n",
    "    y_pred = model(X)      # 1. 喂数据，出结果\n",
    "    loss = criterion(y_pred, y) # 2. 算差距 (Loss)\n",
    "\n",
    "    # --- 反向传播 (Backward) ---\n",
    "    optimizer.zero_grad()  # 3. 梯度归零 (防止上次的梯度残留)\n",
    "    loss.backward()        # 4. 计算梯度 (反向找锅)\n",
    "    optimizer.step()       # 5. 更新权重 (根据梯度调整参数)\n",
    "\n",
    "    # 每 200 次打印一下进度\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/2000], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 5. 测试使用 (Inference)\n",
    "print(\"\\n训练结束，来测试一下预测结果：\")\n",
    "with torch.no_grad(): # 测试时不需要算梯度，省显存\n",
    "    test_input = torch.tensor([[0.0, 1.0]]).to(device) # 测试数据：0 和 1\n",
    "    prediction = model(test_input)\n",
    "    print(f\"输入 [0, 1] -> 预测值: {prediction.item():.4f} (应该是接近 1)\")\n",
    "\n",
    "    test_input2 = torch.tensor([[1.0, 1.0]]).to(device) # 测试数据：1 和 1\n",
    "    prediction2 = model(test_input2)\n",
    "    print(f\"输入 [1, 1] -> 预测值: {prediction2.item():.4f} (应该是接近 0)\")"
   ],
   "id": "b45c57bed8e96ce8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的设备: cuda\n",
      "开始训练...\n",
      "Epoch [200/2000], Loss: 0.0013\n",
      "Epoch [400/2000], Loss: 0.0004\n",
      "Epoch [600/2000], Loss: 0.0002\n",
      "Epoch [800/2000], Loss: 0.0001\n",
      "Epoch [1000/2000], Loss: 0.0001\n",
      "Epoch [1200/2000], Loss: 0.0000\n",
      "Epoch [1400/2000], Loss: 0.0000\n",
      "Epoch [1600/2000], Loss: 0.0000\n",
      "Epoch [1800/2000], Loss: 0.0000\n",
      "Epoch [2000/2000], Loss: 0.0000\n",
      "\n",
      "训练结束，来测试一下预测结果：\n",
      "输入 [0, 1] -> 预测值: 1.0000 (应该是接近 1)\n",
      "输入 [1, 1] -> 预测值: 0.0000 (应该是接近 0)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c08e2b2987fca0ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
