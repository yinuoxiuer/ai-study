{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T08:35:59.437239800Z",
     "start_time": "2026-02-08T08:35:59.417658800Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "3fdcad496c0f8938",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T08:35:59.470867500Z",
     "start_time": "2026-02-08T08:35:59.445749100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "    return 3. * x ** 2 + 2. * x - 1\n",
    "#近视求导，x移动eps单位，也就是离自己很近的一个点的切线\n",
    "def approximate_derivative(f, x, eps=1e-6):\n",
    "    return (f(x + eps) - f(x - eps)) / (2. * eps)\n",
    "\n",
    "print(approximate_derivative(f, 1.))"
   ],
   "id": "de26be5ff33cd7c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999785956\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-08T08:35:59.526737900Z",
     "start_time": "2026-02-08T08:35:59.470867500Z"
    }
   },
   "source": [
    "\n",
    "#求偏导数,其中一个数不动，对另外一个变量求导\n",
    "def g(x1, x2):\n",
    "    return (x1 + 5) * (x2 ** 2)\n",
    "\n",
    "def approximate_gradient(g, x1, x2, eps=1e-3):\n",
    "    dg_x1 = approximate_derivative(lambda x: g(x, x2), x1, eps)\n",
    "    dg_x2 = approximate_derivative(lambda x: g(x1, x), x2, eps)\n",
    "    return dg_x1, dg_x2\n",
    "\n",
    "print(approximate_gradient(g, 2., 3.))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.999999999993236, 41.999999999994486)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "以上是手动实现",
   "id": "4cd0af15f3394a70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T08:46:00.536971100Z",
     "start_time": "2026-02-08T08:46:00.502163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x1 = torch.tensor([2.], requires_grad=True)\n",
    "x2 = torch.tensor([3.], requires_grad=True)\n",
    "y = g(x1, x2)\n",
    "\n",
    "(dy_dx1,) = torch.autograd.grad(y, x1,retain_graph=True)\n",
    "print(dy_dx1)"
   ],
   "id": "ead9b185703bacc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这个是另外一种接口",
   "id": "f8f0a96d7d4ffb57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x1 = torch.tensor([2.], requires_grad=True)\n",
    "x2 = torch.tensor([3.], requires_grad=True)\n",
    "y = g(x1, x2)\n",
    "\n",
    "# 求偏导数,求梯度\n",
    "y.backward()\n",
    "print(x1.grad, x2.grad)"
   ],
   "id": "478acd8cbc8ce34d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "下面是对之前梯度下降的简单展示",
   "id": "7299ca0f51dd9275"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T09:03:28.287722800Z",
     "start_time": "2026-02-08T09:03:28.209901400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#模拟梯度下降算法 SGD\n",
    "import torch\n",
    "learning_rate = 0.3\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "for _ in range(100):\n",
    "    z = f(x)\n",
    "    z.backward()\n",
    "    x.data.sub_(learning_rate * x.grad) # x -= learning_rate * x.grad，这里就等价于optimizer.step()\n",
    "    x.grad.zero_() # x.grad -= x.grad, x.grad = 0,梯度清零\n",
    "print(x)"
   ],
   "id": "6806e169d53338d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3333, requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T09:03:29.496297Z",
     "start_time": "2026-02-08T09:03:28.835408600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#GradientTape与optimizer（优化器）结合使用\n",
    "learning_rate = 0.01\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x], lr=learning_rate,momentum=0.9)\n",
    "for _ in range(500):\n",
    "    z = f(x)\n",
    "    optimizer.zero_grad() # 梯度变为0，这一步看出优化器能够在没有梯度的时候作用\n",
    "    z.backward() # dz/dx,求梯度\n",
    "    # print(x.grad)\n",
    "    optimizer.step() # x -= learning_rate * x.grad\n",
    "\n",
    "\n",
    "print(x)\n"
   ],
   "id": "e208abb8649c1a53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3333, requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4658dcaf02aa770a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
